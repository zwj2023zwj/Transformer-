# ä»é›¶å®ç° Transformer (Transformer from Scratch)

[![Language](https://img.shields.io/badge/Language-Python-blue.svg)](https://www.python.org/)
[![Framework](https://img.shields.io/badge/Framework-PyTorch-orange.svg)](https://pytorch.org/)

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªä»é›¶å¼€å§‹ã€ä¸ä¾èµ–ä»»ä½•é«˜çº§åº“ï¼ˆå¦‚ `transformers`ï¼‰çš„ Transformer æ¨¡å‹å®ç°ï¼Œä¸“æ³¨äºè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åºåˆ—åˆ°åºåˆ—ï¼ˆSeq2Seqï¼‰ä»»åŠ¡ã€‚é€šè¿‡è¯¥é¡¹ç›®ï¼Œå¯ä»¥æ·±å…¥ç†è§£Transformeræ¶æ„çš„æ¯ä¸€ä¸ªæ ¸å¿ƒç»„ä»¶ã€‚

æœ¬é¡¹ç›®åœ¨ç»å…¸çš„**IWSLT 2017 å¾·è¯­-è‹±è¯­**æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸Šå¯¹æ¨¡å‹è¿›è¡Œäº†å®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°ï¼Œå¹¶åŒ…å«äº†è¯¦ç»†çš„æ¶ˆèå®éªŒæ¥éªŒè¯å…³é”®æ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚

---

## âœ¨ ä¸»è¦å†…å®¹

- **æ¨¡å‹æ¶æ„**ï¼š
  - [x] ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› (Scaled Dot-Product Attention)
  - [x] å¤šå¤´è‡ªæ³¨æ„åŠ› (Multi-Head Self-Attention)
  - [x] ä½ç½®ç¼–ç  (Sinusoidal Positional Encoding)
  - [x] é€ä½ç½®å‰é¦ˆç½‘ç»œ (Position-wise Feed-Forward Network)
  - [x] æ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ– (Residual Connection & Layer Normalization)
  - [x] å®Œæ•´çš„ç¼–ç å™¨ (Encoder) å’Œè§£ç å™¨ (Decoder) ç»“æ„
  - [x] æ©ç æœºåˆ¶ (Padding Mask & Look-ahead Mask)

- **è®­ç»ƒç­–ç•¥**ï¼š
  - [x] ä½¿ç”¨ AdamW ä¼˜åŒ–å™¨
  - [x] å­¦ä¹ ç‡è°ƒåº¦ (Cosine Annealing)
  - [x] æ ‡ç­¾å¹³æ»‘ (Label Smoothing)
  - [x] æ—©åœæœºåˆ¶ (Early Stopping)
  - [x] æ”¯æŒåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œè®­ç»ƒ (DDP)

- **å®éªŒå¤ç°**ï¼š
  - [x] æä¾›äº†å®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°è„šæœ¬
  - [x] åŒ…å«äº†è¯¦ç»†çš„è¶…å‚æ•°é…ç½®
  - [x] æä¾›äº†è®­ç»ƒæ›²çº¿å’Œæ¶ˆèå®éªŒç»“æœ

---

## ğŸ“‚ æ–‡ä»¶ç»“æ„

```
Transformer/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ de-en/
â”‚       â”œâ”€â”€ IWSLT17.TED.dev2010.de-en.de.xml
â”‚       â””â”€â”€ ... (å…¶ä»–æ•°æ®é›†æ–‡ä»¶)
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ training_curves.png
â”‚   â”œâ”€â”€ test_metrics.json
â”‚   â”œâ”€â”€ training_validation_ppl_curve.png
â”‚   â”œâ”€â”€ training_stats.csv
â”‚   â””â”€â”€ best_model.pt
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run.sh
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ transformer/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ attention.py       # æ³¨æ„åŠ›æœºåˆ¶å®ç°
â”‚   â”‚   â”œâ”€â”€ layers.py          # åŸºç¡€å±‚å®ç° (FFN, LayerNormç­‰)
â”‚   â”‚   â”œâ”€â”€ encoder.py         # ç¼–ç å™¨å®ç°
â”‚   â”‚   â”œâ”€â”€ decoder.py         # è§£ç å™¨å®ç°
â”‚   â”‚   â””â”€â”€ transformer.py     # Transformeræ¨¡å‹ç»„è£…
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ visualization.py   # å¯è§†åŒ–è„šæœ¬
â”‚   â”‚   â”œâ”€â”€ data_utils.py      # æ•°æ®å¤„ç†å·¥å…·
â”‚   â”‚   â””â”€â”€ train_utils.py     # è®­ç»ƒå·¥å…·
â”‚   â””â”€â”€ train.py               # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

- **å…‹éš†ä»“åº“**:
  ```bash
  git clone https://github.com/zwj2023zwj/Transformer-.git
  cd Transformer-
  ```

- **åˆ›å»º Conda ç¯å¢ƒå¹¶å®‰è£…ä¾èµ–**: (æ¨èä½¿ç”¨ Python 3.9)
  ```bash
  conda create -n transformer python=3.9
  conda activate transformer
  pip install -r requirements.txt
  ```

### 2. æ•°æ®é›†å‡†å¤‡

æœ¬é¡¹ç›®ä½¿ç”¨ IWSLT 2017 De-En æ•°æ®é›†ã€‚è¯·ä» IWSLT å®˜ç½‘ä¸‹è½½æ•°æ®é›†ï¼Œè§£å‹åå°†å…¶ä¸­çš„ `*.de.xml` å’Œ `*.en.xml` ç­‰æ–‡ä»¶æ”¾ç½®äº `data/de-en/` ç›®å½•ä¸‹ã€‚

### 3. å¼€å§‹è®­ç»ƒ

æˆ‘ä»¬æä¾›äº† `run.sh` è„šæœ¬æ¥å¤ç°å®éªŒã€‚è¯¥è„šæœ¬åŒ…å«äº†æŠ¥å‘Šä¸­ä½¿ç”¨çš„æ‰€æœ‰è¶…å‚æ•°ã€‚

- **ç›´æ¥è¿è¡Œè„šæœ¬**:
  ```bash
  bash scripts/run.sh
  ```

- **æˆ–è€…ç›´æ¥æ‰§è¡Œä»¥ä¸‹å‘½ä»¤**:
  è¯¥å‘½ä»¤å°†åœ¨ä¸¤å¼  GPU ä¸Šå¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒï¼Œå¹¶å®Œæ•´å¤ç°æŠ¥å‘Šä¸­çš„å®éªŒã€‚
  ```bash
  torchrun --standalone --nproc_per_node=2 src/train.py \
    --dataset iwslt \
    --data_dir ./data \
    --language_pair de-en \
    --batch_size 256 \
    --epochs 100 \
    --ddp \
    --dist_backend nccl \
    --max_len 100 \
    --d_model 128 \
    --n_layers 3 \
    --n_heads 8 \
    --d_ff 512 \
    --dropout 0.3 \
    --label_smoothing 0.1 \
    --weight_decay 0.02 \
    --patience 5 \
    --save_dir ./results \
    --use_adamw \
    --lr 5e-4 \
    --scheduler cosine \
    --is_pos_encoding True \
    --seed 42
  ```

è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ—¥å¿—ã€æ¨¡å‹æƒé‡å’Œå¯è§†åŒ–å›¾è¡¨å°†ä¿å­˜åœ¨ `results/` ç›®å½•ä¸‹ã€‚

---

## ğŸ“Š å®éªŒç»“æœ

### ä¸»è¦å®éªŒ

åœ¨ IWSLT 2017 De-En ç¿»è¯‘ä»»åŠ¡ä¸Šï¼Œæ¨¡å‹ç»è¿‡çº¦ 100 ä¸ªå‘¨æœŸçš„è®­ç»ƒåï¼Œåœ¨éªŒè¯é›†ä¸Šè¾¾åˆ°äº† **2.5** å·¦å³çš„å›°æƒ‘åº¦ (Perplexity)ï¼ŒéªŒè¯äº†ä»é›¶å®ç°çš„ Transformer æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚

| æŒ‡æ ‡ | è®­ç»ƒé›† | éªŒè¯é›† |
| :--- | :---: | :---: |
| æœ€ç»ˆæŸå¤± (Loss) | 0.90 | 0.925 |
| æœ€ç»ˆå›°æƒ‘åº¦ (PPL) | 2.46 | 2.52 |

**è®­ç»ƒæ›²çº¿:**

![Loss Curve](./results/main_experiment/training_curves.png "è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿")
*å›¾1: è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿*

![Perplexity Curve](./results/main_experiment/training_validation_ppl_curve.png "è®­ç»ƒå’ŒéªŒè¯å›°æƒ‘åº¦æ›²çº¿")
*å›¾2: è®­ç»ƒå’ŒéªŒè¯å›°æƒ‘åº¦æ›²çº¿*


### æ¶ˆèå®éªŒï¼šç§»é™¤ä½ç½®ç¼–ç 

ä¸ºäº†éªŒè¯ä½ç½®ç¼–ç  (Positional Encoding) çš„å…³é”®ä½œç”¨ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹æ¶ˆèå®éªŒï¼Œå³åœ¨å…¶ä»–æ‰€æœ‰è®¾ç½®ä¿æŒä¸å˜çš„æƒ…å†µä¸‹ï¼Œç§»é™¤ä½ç½®ç¼–ç æ¨¡å—ã€‚

**è®­ç»ƒæ›²çº¿:**

![Loss Curve](./results/ablation/training_curves.png "è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿")
*å›¾1: è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿*

![Perplexity Curve](./results/ablation/training_validation_ppl_curve.png "è®­ç»ƒå’ŒéªŒè¯å›°æƒ‘åº¦æ›²çº¿")
*å›¾2: è®­ç»ƒå’ŒéªŒè¯å›°æƒ‘åº¦æ›²çº¿*

**ç»“æœ**:
- éªŒè¯é›†ä¸Šçš„å›°æƒ‘åº¦ (PPL) **æ˜¾è‘—æ¶åŒ–è‡³ 3.0 å·¦å³**ã€‚
- è¿™æœ‰åŠ›åœ°è¯æ˜äº†ä½ç½®ç¼–ç å¯¹äº Transformer æ•æ‰åºåˆ—é¡ºåºä¿¡æ¯è‡³å…³é‡è¦ã€‚æ²¡æœ‰å®ƒï¼Œæ¨¡å‹æ— æ³•åŒºåˆ†è¯åºï¼Œæ€§èƒ½å¤§å¹…ä¸‹é™ã€‚

---

## ğŸ› ï¸ ç¡¬ä»¶ä¸æ€§èƒ½

- **ç¡¬ä»¶ç¯å¢ƒ**: 2 x NVIDIA A40 (48Gæ˜¾å­˜)
- **è®­ç»ƒæ—¶é•¿**: çº¦ **4å°æ—¶10åˆ†é’Ÿ** å®Œæˆ 100 ä¸ªå‘¨æœŸçš„è®­ç»ƒä¸éªŒè¯ã€‚